% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/gbt.train.R
\name{gbt.train}
\alias{gbt.train}
\title{GBTorch Training.}
\usage{
gbt.train(y, x, learning_rate = 0.01, loss_function = "mse",
  nrounds = 50000, verbose = 0, greedy_complexities = TRUE,
  previous_pred = NULL, weights = NULL)
}
\arguments{
\item{y}{response vector for training. Must correspond to the design matrix \code{x}.}

\item{x}{design matrix for training. Must be of type \code{matrix}.}

\item{learning_rate}{control the learning rate: scale the contribution of each tree by a factor of \code{0 < learning_rate < 1} when it is added to the current approximation. Lower value for \code{learning_rate} implies an increase in the number of boosting iterations: low \code{learning_rate} value means model more robust to overfitting but slower to compute. Default: 0.01}

\item{loss_function}{specify the learning objective (loss function). Only pre-specified loss functions are currently supported.
\itemize{
\item \code{mse} regression with squared error loss (Default).
\item \code{logloss} logistic regression for binary classification, output score before logistic transformation.
\item \code{poisson} Poisson regression for count data using a log-link, output score before natural transformation.
\item \code{gamma::neginv} gamma regression using the canonical negative inverse link. Scaling independent of y.
\item \code{gamma::log} gamma regression using the log-link. Constant information parametrisation. 
}}

\item{nrounds}{a just-in-case max number of boosting iterations. Default: 50000}

\item{verbose}{Enable boosting tracing information at i-th iteration? Default: \code{0}.}

\item{greedy_complexities}{Boolean: \code{FALSE} means standard GTB, \code{TRUE} means greedy complexity tree-building. Default: \code{TRUE}.}

\item{previous_pred}{prediction vector for training. Boosted training given predictions from another model.}

\item{weights}{weights vector for scaling contributions of individual observations. Default \code{NULL} (the unit vector).}
}
\value{
An object of class \code{ENSEMBLE} with the following elements:
\itemize{
  \item \code{handle} a handle (pointer) to the gbtorch model in memory.
  \item \code{initialPred} a field containing the initial prediction of the ensemble.
  \item \code{set_param} function for changing the parameters of the ensemble.
  \item \code{get_param} function for looking up the parameters of the ensemble.
  \item \code{train} function for re-training (or from scratch) the ensemble directly on vector \code{y} and design matrix \code{x}.
  \item \code{predict} function for predicting observations given a design matrix
  \item \code{predict2} function as above, but takes a parameter max number of boosting ensemble iterations.
  \item \code{estimate_generalization_loss} function for calculating the (approximate) optimism of the ensemble.
  \item \code{get_num_trees} function returning the number of trees in the ensemble.
}
}
\description{
\code{gbt.train} is an interface for training a \code{gbtorch} model.
}
\details{
These are the training functions for \code{gbtorch}.

Explain the philosophy and the algorithm and a little math

\code{gbt.train} learn trees with adaptive complexity given by an information criterion, 
until the same (but scaled) information criterion tells the algorithm to stop. The data used 
for training at each boosting iteration stems from a second order Taylor expansion to the loss 
function, evaluated at predictions given by ensemble at the previous boosting iteration.

Formally, ....
}
\examples{
## A simple gtb.train example with linear regression:
x <- runif(500, 0, 4)
y <- rnorm(500, x, 1)
x.test <- runif(500, 0, 4)
y.test <- rnorm(500, x.test, 1)

mod <- gbt.train(y, as.matrix(x))
y.pred <- predict( mod, as.matrix( x.test ) )

plot(x.test, y.test)
points(x.test, y.pred, col="red")


}
\references{
B. Ã…. S. Lunde, T. S. Kleppe and H. J. Skaug, "An information criterion for gradient boosted trees"
publishing details, \url{}
}
\seealso{
\code{\link{gbt.pred}}
}
