\section{Background}

%\subsection{Supervised learning and gradient tree boosting}

\begin{frame}{Question 1: Linear regression}
	
	\begin{columns}[T]
		\begin{column}{0.35\textwidth}
			%\begin{figure}
				\includegraphics<1->[height=2.9cm,width=3.5cm]{figures/growth_girls_lm.pdf} 
				\includegraphics<1->[height=2.9cm,width=3.5cm]{figures/growth_boys_lm.pdf}		
			%\end{figure}
		\end{column}
		\begin{column}{0.6\textwidth}
			\begin{myblock}{Researcher asks...}{bg=blue!5,fg=black}{bg=blue!10, fg=black}
				How can I model the \textcolor{orange}{\texttt{height}} of children given their \textcolor{orange}{\texttt{age}} and \textcolor{orange}{\texttt{sex}}?
				And I need a model fast! [Berkeley growth curve dataset]
			\end{myblock}
		\visible<2>{
			\begin{myblock}{The statistician responds...}{bg=yellow!05,fg=black}{bg=yellow!20, fg=black}
				Easy! Just try a linear regression: 
				$\textcolor{orange}{\texttt{height}}\approx \beta_0 + \beta_1 
				\textcolor{orange}{\texttt{age}} + \beta_2 \textcolor{orange}{\texttt{sex}}$.
				Estimate parameters $\mathbf{\beta} = \{\beta_0, \beta_1, \beta_2\}$ by minimizing the mean squared error (MSE):\\ $\hat{\mathbf{\beta}} = \arg\min_\mathbf{\beta} \sum_i \left(y_i - f(\textcolor{orange}{\texttt{age}}_i,\textcolor{orange}{\texttt{sex}}_i;\mathbf{\beta})\right)^2$.
			\end{myblock}}
		\end{column}
	\end{columns}
	
\end{frame}

\begin{frame}{Question 2: Generalized linear models}
	
	\begin{columns}[T]
	\begin{column}{0.35\textwidth}
		%\begin{figure}
		\includegraphics<1->[height=5.5cm,width=4cm]{figures/claims_glm.pdf} 
		%\includegraphics<1->[height=2.9cm,width=3.5cm]{figures/growth_boys_lm.pdf}		
		%\end{figure}
	\end{column}
	\begin{column}{0.6\textwidth}
		\begin{myblock}{Researcher asks...}{bg=blue!5,fg=black}{bg=blue!10, fg=black}
			Is there an efficient way to model the \textcolor{orange}{\texttt{risk}} of customers of insurance given some history of \textcolor{orange}{\texttt{claims}} and information about the customers?
			The model needs to be production friendly!
		\end{myblock}
		\visible<2>{
			\begin{myblock}{The actuary responds...}{bg=yellow!05,fg=black}{bg=yellow!20, fg=black}
				Easy! Divide and conquer: split the \textcolor{orange}{\texttt{claims}} into \textcolor{orange}{\texttt{size}} and \textcolor{orange}{\texttt{frequency}} and model them using a gamma and a Poisson generalized linear model, respectively. The \texttt{glm()}-function in R is your friend.
		\end{myblock}}
	\end{column}
\end{columns}
 
\end{frame}


\begin{frame}{Supervised learning}
	
	\begin{itemize}
		\item The above problems may be framed as \textcolor{orange}{supervised learning}:
	\end{itemize}
	
	\begin{block}{Supervised learning}
		Find the best (in expectation, relative to loss $l$) predictive function:
		\begin{align*}
		\hat{f} = \arg\min_f E_{\features\response}\left[l(\response,f(\features))\right]
		\end{align*}
		The loss often correspond to a nll.
\end{block}

\visible<2->{
\begin{myblock}{User restricted $f$, is it...}{bg=yellow!05,fg=black}{bg=yellow!20, fg=black}
	\begin{itemize}
		\item Non-linear?
		\item Continuous?
		\item Which features should it use?
		\item Do we have enough data to parametrize $f$?
	\end{itemize}}
\end{myblock}
	
	
	
%	\begin{columns}[T]
%		\begin{column}{0.52\textwidth}
%			\begin{block}{Maximum likelihood}
%				LR and GLM both learn their parameters from data:%, by maximizing the likelihood of observing the data, given that the data was generated by the model.
%				\begin{align*}
%					\hat{\beta} = \arg\max_\beta p(\texttt{data}|\beta)
%				\end{align*}
%				\visible<2->{
%				Equivalent to minimizing the nll
%				\begin{align*}
%					\hat{\beta} = \arg\min_\beta \left\{- \sum_i \log (p(\texttt{obs}_i|\beta))\right\}
%				\end{align*}
%				assuming independent observations
%				\begin{itemize}
%					\item Minimizing MSE correspond to minimizing a Gaussian nll.
%				\end{itemize}}
%			\end{block}
%		\end{column}
%		\begin{column}{0.5\textwidth}
%			\visible<3->{
%			\begin{block}{Supervised learning}
%				Find the best (in expectation, relative to loss $l$) predictive function:
%				\begin{align*}
%					\hat{f} = \arg\min E_{\features\response}\left[l(\response,f(\features))\right]
%				\end{align*}
%				
%				The loss often correspond to a nll.
%				\visible<4->{
%				User restricted $f$: 
%				Is it
%				\begin{itemize}
%					\item Non-linear?
%					\item Continuous?
%					\item Which features should it use?
%					\item Do we have enough data to parametrize $f$?
%				\end{itemize}}
%			\end{block}}
%		\end{column}
%	\end{columns}
	
	
	
\end{frame}


\begin{frame}{Question 3: Gradient boosting}
	
	
	\begin{myblock}{Researcher asks...}{bg=blue!5,fg=black}{bg=blue!10, fg=black}
		I want to do well in this ML-competition, but...
		\begin{itemize}
			\item<2-> My data has missing values
			\item<2-> Both $n$ and $p$ are very large (design matrix with some billion elements)
			\item<2-> Relationships are non-linear and possibly discontinuous
			\item<2-> I don't care about explainability, just give me predictive power!
		\end{itemize}
	\end{myblock}
	\visible<3->{\begin{myblock}{The data scientist/Kaggle master responds...}{bg=yellow!05,fg=black}{bg=yellow!20, fg=black}
		Try gradient boosting? 
		\begin{itemize}\small
			\item<3-> State-of-the-art gradient boosting libraries: XGBoost, LightGBM and CatBoost.
		\end{itemize}
	\end{myblock}}

\end{frame}

\begin{frame}{Gradient boosting, what is going on behind the scenes?}
	
	Gradient boosting attacks the supervised learning problem directly
	
	%For many problems coined as supervised learning problems, gradient boosting
	
	\begin{myblock}{}{bg=blue!5,fg=black}{bg=blue!10, fg=black}
		\begin{itemize}
			\item Start with a constant value: $f^{(0)} = \arg\min_\eta \sum_i l(y_i,\eta)$
			\item Iteratively, add $\delta f_k$ to $f^{(k-1)}$, where $f_k$ is trained on the "error" (MSE case) of $f^{(k-1)}$, and $\delta$ is some small number scaling $f_k$.
		\end{itemize}
	\end{myblock}

	\begin{columns}[T]
		\begin{column}{0.35\textwidth}
			% p0
			\includegraphics<1>[height=4.5cm,width=4.5cm]{figures/lm_boost_p.pdf} 
			\includegraphics<2-4>[height=4.5cm,width=4.5cm]{figures/lm_boost_p_0.pdf}
					%p1
			\includegraphics<5-7>[height=4.5cm,width=4.5cm]{figures/lm_boost_p1.pdf}
			%p2
			\includegraphics<8-10>[height=4.5cm,width=4.5cm]{figures/lm_boost_p2.pdf}			
			%p3
			\includegraphics<11-13>[height=4.5cm,width=4.5cm]{figures/lm_boost_p3.pdf}			
			%p4
			\includegraphics<14>[height=4.5cm,width=4.5cm]{figures/lm_boost_p4.pdf}			
		\end{column}
		\begin{column}{0.35\textwidth}
			%res1
			\includegraphics<3>[height=4.5cm,width=4.5cm]{figures/lm_boost_res1.pdf}
			\includegraphics<4-5>[height=4.5cm,width=4.5cm]{figures/lm_boost_resf1.pdf} 
			%res2
			\includegraphics<6>[height=4.5cm,width=4.5cm]{figures/lm_boost_res2.pdf}
			\includegraphics<7-8>[height=4.5cm,width=4.5cm]{figures/lm_boost_resf2.pdf} 
						%res3
			\includegraphics<9>[height=4.5cm,width=4.5cm]{figures/lm_boost_res3.pdf}
			\includegraphics<10-11>[height=4.5cm,width=4.5cm]{figures/lm_boost_resf3.pdf} 
						%res4
			\includegraphics<12>[height=4.5cm,width=4.5cm]{figures/lm_boost_res4.pdf}
			\includegraphics<13-14>[height=4.5cm,width=4.5cm]{figures/lm_boost_resf4.pdf} 
		\end{column}
	\end{columns}

	
\end{frame}

\begin{frame}{Why this iterative procedure is a good idea}
	
	\begin{myblock}{The procedure...}{bg=blue!5,fg=black}{bg=blue!10, fg=black}
		\visible<2->{\begin{itemize}
			\item Adapts the complexity of the model, $f$, to the data,
			\item Only add as much complexity in a certain direction as it deserves
			\item Builds sparse models: Connection to the LARS algorithm for computing LASSO solution paths.
		\end{itemize}}
	\end{myblock}

	\visible<3->{
	\begin{myblock}{It can be generalized beyond MSE:}{bg=yellow!30, fg=black}{bg=yellow!50, fg=black}
	\begin{itemize}
		\item Given a differentiable loss function $l$
		\item Instead of building a model on the "errors" in the MSE case, 
		\item Compute derivatives from $l(y_i,\hat{y}_i)$ over the data given predictions $\hat{y}_i$ from the current model.
		\item Build a model on the derivatives.
	\end{itemize}
	\end{myblock}}
	
\end{frame}

\begin{frame}{Trees: where boosting gets interesting}
	
	We used a linear model for "base learners" $f_k$: 
	\begin{itemize}
		\item The linear combination of linear functions is still a linear model...
	\end{itemize}
	\visible<2->{
	\begin{myblock}{}{bg=blue!5,fg=black}{bg=blue!10, fg=black}
	\begin{itemize}
			\item<2-> More interesting with non-linear learning procedures for $f_k$ %(adapting to error / derivatives)...
			\item<2-> But needs to retain the possibility of a simple (sparse) model.
			\item<2-> We need something that can be non-linear but adapts this to data!
			\item<3-> Trees: complexity from the simple mean or "tree-stumps" to potentially a complete fit to training data.
	\end{itemize}
	\end{myblock}}
	
	%important with splitting algorithm
\end{frame}

\begin{frame}{The tree-learning prcedure: recursive binary splitting}
	
	
	Trees are constant predictions in $T$ regions, $R_t$, of feature space:
	\begin{columns}[T]
		\begin{column}{0.6\textwidth}
			\begin{align*}
			\pred = \sum_{t=1}^{T} w_t I(\features \in R_t)
			\end{align*}
			But how do we choose the regions $R_t$?
		\end{column}
		\begin{column}{0.39\textwidth}
			\begin{tikzpicture}
			\draw (0,0)coordinate(A) -- node[below] {$X_1$}  ++ (3,0)coordinate(B) -- (3,3)coordinate(C) -- (0,3)coordinate(D) -- node[left]{$X_2$} cycle;
			
			% initial figure
			\draw[name path=ef] (0,1)coordinate(E) -- (3,1)coordinate(F);
			\draw[name path=gh] (1,1)coordinate(G) -- (1,3)coordinate(H);
			\draw (1,2)coordinate(I) -- (3,2)coordinate(J);
			
			\fill[blue!50](A)--(B)--(F)--(E)--cycle;
			\fill[orange](E)--(G)--(H)--(D)--cycle;
			\fill[red](G)--(F)--(J)--(I)--cycle;
			\fill[green] (I)--(J)--(C)--(H)--cycle;
			
			\node[fill=blue!50,circle,text=black] at (barycentric cs:A=1,B=1,F=1 ,E=1) {$R_1$};
			\node[text=black] at (barycentric cs:E=1,G=1,H=1 ,D=1) {$R_2$};
			\node[text=black] at (barycentric cs:G=1,F=1,J=1 ,I=1) {$R_3$};
			\node[text=black] at (barycentric cs:I=1,J=1,C=1 ,H=1) {$R_4$};	
			
			%split points
			\node[right=0.2cm of F,visible on=<3->]{$s_2^{(1)}$};		
			\node[above=0.2cm of H,visible on=<4->]{$s_1^{(2)}$};
			\node[right=0.2cm of J,visible on=<5->]{$s_2^{(3)}$};
			
						
			%\draw (0, 0) -- node[below]{$X_1$} -- (0, 3)coordinate(B)node[above left]{B} -- (3, 3)coordinate(C)node[above right]{C} -- (3, 0)coordinate(D)node[below right]{D} --cycle;
			%\draw[name path=ae] (B) -- (2, 1)coordinate(E)node[right]{E};
			%\draw[name path=ac] (A) -- (C);
			%\path[name intersections ={of= ae and ac,by=I}];
			%\fill[green](A)--(B)--(I)--cycle;
			%\fill[blue](B)--(C)--(I)--cycle;
			%\fill[red](C)--(E)--(I)--cycle;
			%\fill[violet](A)--(D)--(E)--(I)--cycle;
			\end{tikzpicture}
		\end{column}
	\end{columns}

	
			\visible<2->{\begin{myblock}{Recursive binary splitting}{bg=blue!5,fg=black}{bg=blue!10, fg=black}
				\begin{enumerate}
					\item Start with a constant prediction for all of feature space
					\item Split a leaf-node into two regions (on feature $j$ and split-point $s_j$ chosen by some criteria).
					\item Continue step 2 recursively on all leaves.
				\end{enumerate}
			\end{myblock}
		}
	
\end{frame}

\begin{frame}{Second order gradient tree boosting}
	
	\begin{myblock}{}{bg=blue!5,fg=black}{bg=blue!10, fg=black}
	Iteratively add $\delta f_k$ where $f_k$ are trees trained on derivatives
	\begin{align*}
	g_{ik} =  \left.\frac{\partial}{\partial \pred_i} \loss(\response_i, \pred_i)\right|_{\substack{\pred_i = f^{(k-1)}(\features_i)}}
	\text{ and }
	h_{ik} = \left.\frac{\partial^2}{\partial \pred_i^2} \loss(\response_i, \pred_i)\right|_{\substack{\pred_i = f^{(k-1)}(\features_i)}}.
	\end{align*}
	\end{myblock}
	
		\begin{columns}[T]
		\begin{column}{0.35\textwidth}
			% p0
			\includegraphics<1>[height=4.5cm,width=4.5cm]{figures/tree_boost_p.pdf} 
			\includegraphics<2-4>[height=4.5cm,width=4.5cm]{figures/tree_boost_p_0.pdf}
			%p1
			\includegraphics<5-7>[height=4.5cm,width=4.5cm]{figures/tree_boost_p1.pdf}
			%p2
			\includegraphics<8-10>[height=4.5cm,width=4.5cm]{figures/tree_boost_p2.pdf}			
			%p3
			\includegraphics<11-13>[height=4.5cm,width=4.5cm]{figures/tree_boost_p3.pdf}			
			%p4
			\includegraphics<14>[height=4.5cm,width=4.5cm]{figures/tree_boost_p4.pdf}
		\end{column}
		\begin{column}{0.35\textwidth}
			%res1
			\includegraphics<3>[height=4.5cm,width=4.5cm]{figures/tree_boost_res1.pdf}
			\includegraphics<4-5>[height=4.5cm,width=4.5cm]{figures/tree_boost_resf1.pdf} 
			%res2
			\includegraphics<6>[height=4.5cm,width=4.5cm]{figures/tree_boost_res2.pdf}
			\includegraphics<7-8>[height=4.5cm,width=4.5cm]{figures/tree_boost_resf2.pdf} 
			%res3
			\includegraphics<9>[height=4.5cm,width=4.5cm]{figures/tree_boost_res3.pdf}
			\includegraphics<10-11>[height=4.5cm,width=4.5cm]{figures/tree_boost_resf3.pdf} 
			%res4
			\includegraphics<12>[height=4.5cm,width=4.5cm]{figures/tree_boost_res4.pdf}
			\includegraphics<13-14>[height=4.5cm,width=4.5cm]{figures/tree_boost_resf4.pdf} 
		\end{column}
	\end{columns}

	
\end{frame}

\begin{frame}{Second order gradient tree boosting: Complexity}
	\begin{myblock}{This was too easy!}{bg=blue!5,fg=black}{bg=blue!10, fg=black}
		\begin{itemize}
			\item How do we choose the complexity of the trees?
			\item And how many boosting iterations?
		\end{itemize}
\end{myblock}	
	\begin{columns}[T]
		\begin{column}{0.35\textwidth}
		\includegraphics<2->[height=4.5cm,width=4.5cm]{figures/xgb_boost_p.pdf}			
		\end{column}
		\begin{column}{0.6\textwidth}
			\visible<3->{
			\begin{myblock}{Regularization}{bg=yellow!05,fg=black}{bg=yellow!20, fg=black}
			\begin{itemize}
			\item Choose a maximum depth?
			\item A maximum number of leaf-nodes?
			\item A minimum observations in node?
			\item A minimum reduction in loss when splitting?
			\item A set number of boosting iterations?
			\end{itemize}
			\end{myblock}	
			}
		\end{column}
	\end{columns}
	
\end{frame}


\begin{frame}{The researcher contemplates}
	
	\begin{myblock}{The researcher goes home...}{bg=blue!5,fg=black}{bg=blue!10, fg=black}
		He is determined to win that ML-competition!...
		\begin{itemize}
			\item<2-> But he only has the 2-gb ram laptop running Windows XP that his job graciously gave him as his every-day workhorse.
		\end{itemize}
	\end{myblock}

	\visible<3->{
		
	\begin{columns}[T]
		
		\begin{column}{0.6\linewidth}
			
			\begin{myblock}{So what does he do?}{bg=yellow!05,fg=black}{bg=yellow!20, fg=black}
				\begin{itemize}
					\item<4-> Opt 1: Buy a new computer?
					\item<5-> Opt 2: Expert knowledge on data and tuning?
					\item<6-> Opt 3: Get lucky?
					\item<7-> Opt 4: Rebuild the algorithm to not require tuning?
				\end{itemize}
			\end{myblock}
			
		\end{column}
	
		\begin{column}{0.4\linewidth}
			\visible<8->{
			\begin{myblock}{\small{Plot twist: the researcher is me!}}{bg=green!05,fg=black}{bg=green!20, fg=black}
				\begin{itemize}
					\item<9-> Opt 1: I am a PhD student...
					\item<10-> Opt 2: I am too lazy to be an expert!
					\item<11-> Opt 3: I like good expectations.
					\item<12-> Opt 4: Hmm...
				\end{itemize}
			\end{myblock}
		}
						
		\end{column}

		
	\end{columns}
		
	}
	
\end{frame}

%\subsection{Model complexity selection}

