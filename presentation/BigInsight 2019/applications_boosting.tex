\section{Applications to the boosting algorithm}

\begin{frame}{Going back to the original idea}
	
	
	\begin{myblock}{Our hope was to...}{bg=yellow!05,fg=black}{bg=yellow!20, fg=black}
		\begin{enumerate}
			\item<1-> Adaptively control the complexity of each tree
			\item<1-> Automatically stop the boosting procedure
			%\item<4-> In a highly efficient manner
		\end{enumerate}
	\end{myblock}
	
	\visible<2->{
	\begin{myblock}{What we do: Two inequalities}{bg=green!05,fg=black}{bg=green!20, fg=black}
	\begin{enumerate}
		\item<2-> For two hierarchical trees, $q^0$ and $q^1$, where $q^1$ holds one more split than $q^0$, don't split if
		\begin{align*}
		\E\left[\hat{l}(\response,f(\features;\hat{\mathbf{w}}^0,\hat{q}^0)) - \hat{l}(\response,f(\features;\hat{\mathbf{w}}^1,\hat{q}^1))\right] + C_m(\hat{\mathbf{w}}^0,\hat{q}^0)
		 - C_m(\hat{\mathbf{w}}^1,\hat{q}^1)
		\end{align*}
		is smaller than zero.
		\item<3-> Stop the iterative boosting algorithm when
		\begin{align*}
		\frac{\delta\left(\delta - 2\right)}{2n}  \sum_{t\in\setleafnodes_k}\frac{G_{tk}^2}{H_{tk}}
		+ \delta C_m\left(\hat{\mathbf{w}}_{t,k},q_{t,k}\right) > 0.
		\end{align*}
	\end{enumerate}
	\end{myblock}
}

	
\end{frame}

\begin{frame}{The algorithm}
%	\resizebox{10cm}{8cm}{
	\small\begin{tabbing}
	\hspace{2em} \= \hspace{2em} \= \hspace{2em} \= \\
	{\bfseries Input}: \\
	\> - A training set $\data_n=\{(x_i, y_{i})\}_{i=1}^n$,\\
	\> - a differentiable loss $l(y,f(x))$,\\
	\> - a learning rate $\delta$,\\
	\> \colorbox{blue!20}{- boosting iterations $K$},\\
	\> \colorbox{blue!20}{- one or more tree-complexity regularization criteria.}\\
	
	1. Initialize model with a constant value:
		$f^{(0)}(\features) = \hat{\eta}= \underset{\eta}{\arg\min} \sum_{i=1}^n \loss(y_i, \eta).$\\
	
	2. \colorbox{blue!20}{{\bfseries for} $k = 1$ to $K$:} \colorbox{orange!30}{{\bfseries while} the inequality (2) evaluates to \textbf{\texttt{false}} } \\
	
	\>	$i)$ Compute derivatives $g_i$ and $h_i$ for all $i=1:n$.\\
	
	\> $ii)$ Determine $q_k$ by the iterative binary splitting procedure until\\
	\>\> \colorbox{blue!20}{a regularization criterion is reached.} \colorbox{orange!30}{ the inequality (1) is \textbf{\texttt{true}} }\\ 
	
	\> $iii)$ Fit the leaf weights $\mathbf{w}$, given $q_k$\\
	
	\>	$v)$ Update the model with a scaled tree:
		$f^{(k)}(\features) = f^{(k-1)}(\features) + \delta f_k(\features).$\\
	{\bfseries end \colorbox{blue!20}{for} \colorbox{orange!30}{while}}\\
	
	3. Output the model: {\bfseries Return} $f^{(K)}(\features)$.%=\sum_{k=0}^{K}f_k(\features)$.\\
	
\end{tabbing}

\end{frame}

\begin{frame}{Does it work?}
	
	\begin{itemize}
		\item The tree-boosting animation in the introduction was generated by this algorithm.
	\end{itemize}
\visible<2->{
	\begin{figure}
		\centering
		\includegraphics<2->[height=4.5cm,width=7cm]{figures/loss_vs_numtrees.pdf}			
		\caption{Training (black) and test loss (orange) and estimated generalization error (blue), for a tree-boosting ensemble trained on 1000 observations from a linear model: $\response\sim N(\features, 1)$. The blue line visualizes inequality 2.}
	\end{figure}
	}
\end{frame}


\begin{frame}{ISLR and ESL datasets}
	
	\begin{itemize}
		\item Comparisons on real data
		\item Every dataset randomly split into training and test datasets 100 different ways
		\item Average test scores (relative to XGB) and standard deviations (parenthesis) 
	\end{itemize}
		
\centering\resizebox{10cm}{2cm}{		
		\begin{tabular}{@{\extracolsep{4pt}}lcccccc} 
		\\[-1.8ex]\hline 
		\hline \\[-1.8ex] 
		Dataset & \multicolumn{1}{c}{Dimensions} & \multicolumn{1}{c}{GBTorch} &  \multicolumn{1}{c}{GLM} & \multicolumn{1}{c}{Random forest} & \multicolumn{1}{c}{XGBoost} \\ 
		\hline \\[-1.8ex] 
		Boston  & $506 \times 14$  & 1.07 (0.162)  &  1.3 (0.179)  & 0.876 (0.15)  & 1 (0.176)   \\  
		Ozone  & $111\times 4$  &0.827 (0.22)  &  0.666 (0.131)  & 0.669 (0.182)  & 1 (0.202)   \\  
		Auto  & $392\times 311$  &1.16 (0.136)  &  11.1 (14.5)  & 0.894 (0.134)  & 1 (0.188)   \\  
		Carseats  &  $400\times 12$  & 1.2 (0.168)  &  0.413 (0.0432)  & 1.16 (0.141)  & 1 (0.115)   \\  
		College   & $777\times 18$& 1.3 (0.948)  & 0.55 (0.154)  & 1.07 (0.906)  & 1 (0.818)   \\  
		Hitters   & $263\times 20$ & 1.05 (0.362)  &  1.21 (0.347)  & 0.796 (0.31)  & 1 (0.318)   \\  
		Wage  & $3000\times 26$ & 1.96 (1.72)  &  289 (35.4)  & 82.2 (21.3)  & 1 (1.01)   \\ 
		Caravan  & $5822\times 86$ & 1.02 (0.0508) & 1.12 (0.115)  & 1.31 (0.168)  & 1 (0.0513)   \\  
		Default  & $10000\times 4$ & 0.938 (0.068)  & 0.902 (0.0698)  & 2.83 (0.51)  & 1 (0.0795)   \\  
		OJ  & $1070\times 18$ & 0.996 (0.0496)  & 0.952 (0.0721)  & 1.17 (0.183)  & 1 (0.0703)   \\  
		Smarket  & $1250\times 7$ & 0.999 (0.00285)  &  1 (0.00651)  & 1.04 (0.0164)  & 1 (0.00259)   \\  
		Weekly  & $1089\times 7$	 & 0.992 (0.0082)  &  0.995 (0.0123)  & 1.02 (0.0195)  & 1 (0.00791)   \\  
		\hline \\[-1.8ex] 
	\end{tabular} 
}
\end{frame}

\begin{frame}{Computational considerations}
	
	\begin{myblock}{In general...}{bg=yellow!05,fg=black}{bg=yellow!20, fg=black}
	\begin{itemize}
		\item Let $k$-fold cross validation be used to determine the tuning for a standard tree-boosting implementation using "early-stopping".
		\item Consider $p$ hyperparameters, each having $r$ candidate values.
		\item<1-> Then our implementation is approximately $k\times r^p +1 $ times faster.
	%	\item<2-> Should give similar results to $p=4$ (tree-complexity criteria).
	\end{itemize}
	\end{myblock}
\visible<2->{
	\begin{myblock}{A comparison with XGB}{bg=green!05,fg=black}{bg=green!20, fg=black}
		\begin{itemize}
			\item<2-> On the Caravan dataset ($5822\times 86$ classification), our implementation took 2.68 seconds to train.
			\item<3-> Using a 30\% validation set, XGB took 4.85 seconds
			\item<4-> One minute using $10$-fold CV: the number of boosting iterations
			\item<5-> About 16 minutes to learn one additional hyperparameter
			\item<6> About 2.65 hours on yet another additional hyperparameter
		\end{itemize}
	\end{myblock}
	}
\end{frame}


% LAST SLIDE in Applications
\begin{frame}{The researcher enters the ML competition}
	
	\begin{itemize}
		\item Would he win?
	\end{itemize}

\visible<2->{
	\begin{myblock}{There are additional techniques for improvement}{bg=blue!5,fg=black}{bg=blue!10, fg=black}
	Most notably...
	\begin{itemize}
	%	\item<2-> Most notably...
		\item<3-> L1-L2 regularization
		\item<3-> Stochastic sampling of both rows and columns
		\item<3-> Our trees are optimal if they all were the last (unscaled) tree
	\end{itemize}
	\end{myblock}
	}
\visible<4->{
	\begin{myblock}{But there are benefits!}{bg=yellow!05,fg=black}{bg=yellow!20, fg=black}
		%But he would have an advantage
		\begin{itemize}
			\item The key to many ML competitions is the feature engineering
			\item Possibility of very quickly (and automatically) testing for relevant features
		\end{itemize}
	\end{myblock}
}
\end{frame}