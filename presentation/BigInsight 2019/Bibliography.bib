% GRADIENT BOOSTING%
@article{friedman2001greedy,
	title={Greedy function approximation: a gradient boosting machine},
	author={Friedman, Jerome H},
	journal={Annals of statistics},
	pages={1189--1232},
	year={2001},
	publisher={JSTOR}
}
@article{friedman2002stochastic,
	title={Stochastic gradient boosting},
	author={Friedman, Jerome H},
	journal={Computational Statistics \& Data Analysis},
	volume={38},
	number={4},
	pages={367--378},
	year={2002},
	publisher={Elsevier}
}
@article{friedman2000additive,
	title={Additive logistic regression: a statistical view of boosting (with discussion and a rejoinder by the authors)},
	author={Friedman, Jerome and Hastie, Trevor and Tibshirani, Robert and others},
	journal={The annals of statistics},
	volume={28},
	number={2},
	pages={337--407},
	year={2000},
	publisher={Institute of Mathematical Statistics}
}
@inproceedings{mason2000boosting,
	title={Boosting algorithms as gradient descent},
	author={Mason, Llew and Baxter, Jonathan and Bartlett, Peter L and Frean, Marcus R},
	booktitle={Advances in neural information processing systems},
	pages={512--518},
	year={2000}
}
@article{mason1999boosting,
	title={Boosting algorithms as gradient descent in function space (Technical Report)},
	author={Mason, L and Baxter, J and Bartlett, P and Frean, M},
	journal={RSISE, Australian National University},
	year={1999}
}
@techreport{breiman1997arcing,
	title={Arcing the edge},
	author={Breiman, Leo},
	year={1997},
	institution={Technical Report 486, Statistics Department, University of California at Berkeley}
}

% SOFTWARE
@Manual{Rlanguage2018,
	title = {R: A Language and Environment for Statistical Computing},
	author = {{R Core Team}},
	organization = {R Foundation for Statistical Computing},
	address = {Vienna, Austria},
	year = {2018},
	url = {https://www.R-project.org/},
}
@inproceedings{chen2016xgboost,
	title={Xgboost: A scalable tree boosting system},
	author={Chen, Tianqi and Guestrin, Carlos},
	booktitle={Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining},
	pages={785--794},
	year={2016},
	organization={ACM}
}
@inproceedings{ke2017lightgbm,
	title={Lightgbm: A highly efficient gradient boosting decision tree},
	author={Ke, Guolin and Meng, Qi and Finley, Thomas and Wang, Taifeng and Chen, Wei and Ma, Weidong and Ye, Qiwei and Liu, Tie-Yan},
	booktitle={Advances in Neural Information Processing Systems},
	pages={3146--3154},
	year={2017}
}
@Manual{h2o2018R,
	title = {h2o: R Interface for 'H2O'},
	author = {Erin LeDell and Navdeep Gill and Spencer Aiello and Anqi Fu and Arno Candel and Cliff Click and Tom Kraljevic and Tomas Nykodym and Patrick Aboyoun and Michal Kurka and Michal Malohlava},
	year = {2018},
	note = {R package version 3.20.0.2},
	url = {https://CRAN.R-project.org/package=h2o},
}
@Manual{gbm2018R,
	title = {gbm: Generalized Boosted Regression Models},
	author = {Greg Ridgeway with contributions from others},
	year = {2017},
	note = {R package version 2.1.3},
	url = {https://CRAN.R-project.org/package=gbm},
}
@Article{rcpp2011R,
	title = {{Rcpp}: Seamless {R} and {C++} Integration},
	author = {Dirk Eddelbuettel and Romain Fran\c{c}ois},
	journal = {Journal of Statistical Software},
	year = {2011},
	volume = {40},
	number = {8},
	pages = {1--18},
	url = {http://www.jstatsoft.org/v40/i08/},
	doi = {10.18637/jss.v040.i08},
}
@Manual{data_table2018R,
	title = {data.table: Extension of `data.frame`},
	author = {Matt Dowle and Arun Srinivasan},
	year = {2018},
	note = {R package version 1.11.4},
	url = {https://CRAN.R-project.org/package=data.table},
}
@Manual{data_table2018R,
	title = {data.tree: General Purpose Hierarchical Data Structure},
	author = {Christoph Glur},
	year = {2018},
	note = {R package version 0.7.6},
	url = {https://CRAN.R-project.org/package=data.tree},
}


% RANDOM FOREST
@inproceedings{ho1995random,
	title={Random decision forests},
	author={Ho, Tin Kam},
	booktitle={Document analysis and recognition, 1995., proceedings of the third international conference on},
	volume={1},
	pages={278--282},
	year={1995},
	organization={IEEE}
}
@article{breiman2001random,
	title={Random forests},
	author={Breiman, Leo},
	journal={Machine learning},
	volume={45},
	number={1},
	pages={5--32},
	year={2001},
	publisher={Springer}
}

% SADDLEPOINT
@book{butler2007saddlepoint,
	title={Saddlepoint approximations with applications},
	author={Butler, Ronald W},
	volume={22},
	year={2007},
	publisher={Cambridge University Press}
}
@article{davison1988saddlepoint,
	title={Saddlepoint approximations in resampling methods},
	author={Davison, Anthony C and Hinkley, David V},
	journal={Biometrika},
	volume={75},
	number={3},
	pages={417--431},
	year={1988},
	publisher={Oxford University Press}
}
@article{daniels1991saddlepoint,
	title={Saddlepoint approximation for the studentized mean, with an application to the bootstrap},
	author={Daniels, HE and Young, GA},
	journal={Biometrika},
	volume={78},
	number={1},
	pages={169--179},
	year={1991},
	publisher={Oxford University Press}
}
@article{field1982location,
	title={Small-sample asymptotic distributions of M-estimators of location},
	author={Field, Christopher A and Hampel, Frank R},
	journal={Biometrika},
	volume={69},
	number={1},
	pages={29--46},
	year={1982},
	publisher={Oxford University Press}
}
@article{field1982small,
	title={Small sample asymptotic expansions for multivariate $ M $-estimates},
	author={Field, Christopher and others},
	journal={The Annals of Statistics},
	volume={10},
	number={3},
	pages={672--689},
	year={1982},
	publisher={Institute of Mathematical Statistics}
}


% MODEL SELECTION CRITERIA
@article{stone1974cross,
	title={Cross-validatory choice and assessment of statistical predictions},
	author={Stone, Mervyn},
	journal={Journal of the royal statistical society. Series B (Methodological)},
	pages={111--147},
	year={1974},
	publisher={JSTOR}
}
@article{akaike1974new,
	title={A new look at the statistical model identification},
	author={Akaike, Hirotugu},
	journal={IEEE transactions on automatic control},
	volume={19},
	number={6},
	pages={716--723},
	year={1974},
	publisher={Ieee}
}
@article{murata1994network,
	title={Network information criterion-determining the number of hidden units for an artificial neural network model},
	author={Murata, Noboru and Yoshizawa, Shuji and Amari, Shun-ichi},
	journal={IEEE Transactions on Neural Networks},
	volume={5},
	number={6},
	pages={865--872},
	year={1994},
	publisher={IEEE}
}
@book{burnham2003model,
	title={Model selection and multimodel inference: a practical information-theoretic approach},
	author={Burnham, Kenneth P and Anderson, David R},
	year={2003},
	publisher={Springer Science \& Business Media}
}
@article{takeuchi1976distribution,
	title={Distribution of information statistics and validity criteria of models},
	author={Takeuchi, Kei},
	journal={Mathematical Science},
	volume={153},
	pages={12--18},
	year={1976}
}
%eic
@article{konishi1996generalised,
	title={Generalised information criteria in model selection},
	author={Konishi, Sadanori and Kitagawa, Genshiro},
	journal={Biometrika},
	volume={83},
	number={4},
	pages={875--890},
	year={1996},
	publisher={Oxford University Press}
}
@article{ishiguro1997bootstrapping,
	title={Bootstrapping log likelihood and EIC, an extension of AIC},
	author={Ishiguro, Makio and Sakamoto, Yosiyuki and Kitagawa, Genshiro},
	journal={Annals of the Institute of Statistical Mathematics},
	volume={49},
	number={3},
	pages={411--434},
	year={1997},
	publisher={Springer}
}
@article{liquet2003bootstrap,
	title={Bootstrap choice of estimators in parametric and semiparametric families: an extension of EIC},
	author={Liquet, B and Sakarovitch, C and Commenges, D},
	journal={Biometrics},
	volume={59},
	number={1},
	pages={172--178},
	year={2003},
	publisher={Wiley Online Library}
}
@article{shimodaira2018information,
	title={An information criterion for model selection with missing data via complete-data divergence},
	author={Shimodaira, Hidetoshi and Maeda, Haruyoshi},
	journal={Annals of the Institute of Statistical Mathematics},
	volume={70},
	number={2},
	pages={421--438},
	year={2018},
	publisher={Springer}
}